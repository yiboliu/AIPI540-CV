{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f4jhHp3KvexW"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "import csv\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains the code for training the non-DL model. The thought here is\n",
        "1. extract all the keypoints and corresponding descriptors in the image\n",
        "2. find out if the keypoints are in or out of bboxes; if in a box, label the keypoint as 1; and 0 otherwise.\n",
        "3. use the descriptor as the feature to train a SVM model.\n",
        "\n",
        "I thought about using bboxes as the label. The challenge is each bbox contains a different number of keypoints and thus a different number of descriptors. If I regard each bbox as a different label, then there will be too many classes, each class with a limited number of observations, which makes the model too complex and inefficient.\n",
        "\n",
        "The output of the current approach would be an image with marked keypoints which are predicted to be wheat. To reduce the complexity and improve efficiency of this model, I used the resized pictures from the training of DL model, set the total number of bbox to be 30000, and limit the nfeatures of orb to be 100. The reason I didn't choose SURF was the restriction of patented functions."
      ],
      "metadata": {
        "id": "iBbZorHZIdzr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zqaoit8x99e",
        "outputId": "8e8d79c7-bfe1-442f-abeb-5acc8bb62d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = 'resized.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as zipped:\n",
        "  zipped.extractall()\n",
        "  print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data():\n",
        "  features = []\n",
        "  labels = []\n",
        "\n",
        "  orb = cv2.ORB_create(scoreType=cv2.ORB_FAST_SCORE, nlevels=1, nfeatures=100) # Use nfeature=1 to reduce the complexity of data\n",
        "\n",
        "  # convert the string of bbox like [1.0, 2.0, 3.0, 4.0] to a list of integers, in which elements are 1/4 of the original float values, to match the resizing\n",
        "  def process_bbox(bbox):\n",
        "    nums = bbox[1:len(bbox)-1].split(', ')\n",
        "    return int(float(nums[0]) * 0.25), int(float(nums[1]) * 0.25), int(float(nums[2]) * 0.25), int(float(nums[3]) * 0.25)\n",
        "\n",
        "  # this image_bbox will be {'image_id':[list of bboxes]} for extracting features and labels\n",
        "  image_bbox = {}\n",
        "\n",
        "  with open('train.csv', 'r') as f:\n",
        "    # read the rows as dicts line by line\n",
        "    reader = csv.DictReader(f)\n",
        "    cnt = 1\n",
        "    for row in reader:\n",
        "      if cnt > 30000: # had to use 30000 as the upper limit, otherwise training would be too slow\n",
        "        break\n",
        "      cnt += 1\n",
        "      img_id = row['image_id']\n",
        "      if img_id not in image_bbox:\n",
        "        image_bbox[img_id] = []\n",
        "      image_bbox[img_id].append(process_bbox(row['bbox'])) # Add the bboxes of this image to the image_bbox dict, corresponding to the key image_id\n",
        "\n",
        "  for img in image_bbox:\n",
        "    image_file = cv2.imread(f\"resized/{img}.jpg\", cv2.IMREAD_GRAYSCALE) # descriptor extractors can only take grayscale images. ref: https://answers.opencv.org/question/155/do-inputs-of-descriptor-extractors-are-required-to-be-grayscale/\n",
        "    keypoints, descriptors = orb.detectAndCompute(image_file, None)\n",
        "    bbox_list = image_bbox[img]\n",
        "    for kpt, des in zip(keypoints, descriptors):\n",
        "      x, y = kpt.pt\n",
        "      inside_bbox = any([x >= bbox[0] and y >= bbox[1] and x <= bbox[0] + bbox[2] and y <= bbox[1] + bbox[3] for bbox in bbox_list]) # to determine if the keypoint is inside any of the bboxes\n",
        "      features.append(des)\n",
        "      # append the labels based on their relative location to bboxes\n",
        "      if not inside_bbox:\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)\n",
        "  return features, labels"
      ],
      "metadata": {
        "id": "01lxU-CnHGL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_save_model(features, labels):\n",
        "  X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=0)\n",
        "\n",
        "  svm = SVC()\n",
        "  svm.fit(X_train, y_train)\n",
        "  y_pred = svm.predict(X_val)\n",
        "\n",
        "  accuracy = accuracy_score(y_val, y_pred)\n",
        "  print(f'accuracy: {accuracy}')\n",
        "  with open('svm_model.pkl', 'wb') as file:\n",
        "    pickle.dump(svm, file)\n",
        "# The final accuracy is ~72%"
      ],
      "metadata": {
        "id": "M6Migc8qNkoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = prepare_data()\n",
        "train_and_save_model(features, labels)"
      ],
      "metadata": {
        "id": "eHbqNbmj6wtt"
      },
      "execution_count": 8,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}